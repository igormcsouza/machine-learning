{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Logistic Regression ***\n",
    "\n",
    "# What I'm trying to do now is to figure out wheter the number is prime or not\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "from numpy import reshape\n",
    "from others import isPrime, isEven\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66, 66, 33, 33\n"
     ]
    }
   ],
   "source": [
    "Xt = [i for i in range(1, 100)]\n",
    "Yt = [[0, 1] if isPrime(i) else [1, 0] for i in Xt]\n",
    "x_train = reshape(Xt,(-1,1))\n",
    "y_train = reshape(Yt, (-1, 2))\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size = 0.33, \n",
    "                                                    random_state = 1)\n",
    "print(\"{0}, {1}, {2}, {3}\".format(len(x_train), len(y_train), len(x_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66 samples, validate on 33 samples\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.4040 - acc: 0.2576 - val_loss: 12.3324 - val_acc: 0.2424\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 0s 155us/step - loss: 11.3941 - acc: 0.2576 - val_loss: 12.3313 - val_acc: 0.2424\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 0s 113us/step - loss: 11.3887 - acc: 0.2576 - val_loss: 12.3300 - val_acc: 0.2424\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 0s 208us/step - loss: 11.3798 - acc: 0.2576 - val_loss: 12.3294 - val_acc: 0.2424\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 0s 211us/step - loss: 11.3757 - acc: 0.2576 - val_loss: 12.3289 - val_acc: 0.2424\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 0s 188us/step - loss: 11.3726 - acc: 0.2576 - val_loss: 12.3283 - val_acc: 0.2424\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 0s 152us/step - loss: 11.3640 - acc: 0.2576 - val_loss: 12.3278 - val_acc: 0.2424\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 0s 129us/step - loss: 11.3605 - acc: 0.2576 - val_loss: 12.3272 - val_acc: 0.2424\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 0s 119us/step - loss: 11.3568 - acc: 0.2576 - val_loss: 12.3266 - val_acc: 0.2424\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 0s 153us/step - loss: 11.3533 - acc: 0.2576 - val_loss: 12.3260 - val_acc: 0.2424\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 0s 296us/step - loss: 11.3494 - acc: 0.2576 - val_loss: 12.3254 - val_acc: 0.2424\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 0s 137us/step - loss: 11.3456 - acc: 0.2576 - val_loss: 12.3242 - val_acc: 0.2424\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 0s 128us/step - loss: 11.3378 - acc: 0.2576 - val_loss: 12.3235 - val_acc: 0.2424\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 0s 78us/step - loss: 11.3329 - acc: 0.2576 - val_loss: 12.3232 - val_acc: 0.2424\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 0s 164us/step - loss: 11.3279 - acc: 0.2576 - val_loss: 12.3229 - val_acc: 0.2424\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 0s 124us/step - loss: 11.3259 - acc: 0.2576 - val_loss: 12.3222 - val_acc: 0.2424\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 0s 109us/step - loss: 11.3212 - acc: 0.2576 - val_loss: 12.3219 - val_acc: 0.2424\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 0s 97us/step - loss: 11.3191 - acc: 0.2576 - val_loss: 12.3211 - val_acc: 0.2424\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 0s 97us/step - loss: 11.3137 - acc: 0.2576 - val_loss: 12.3208 - val_acc: 0.2424\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 0s 145us/step - loss: 11.3121 - acc: 0.2576 - val_loss: 12.3202 - val_acc: 0.2424\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 0s 186us/step - loss: 11.3076 - acc: 0.2576 - val_loss: 12.3199 - val_acc: 0.2424\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 0s 132us/step - loss: 11.3038 - acc: 0.2576 - val_loss: 12.3196 - val_acc: 0.2424\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 0s 172us/step - loss: 11.3022 - acc: 0.2576 - val_loss: 12.3193 - val_acc: 0.2424\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 0s 102us/step - loss: 11.3001 - acc: 0.2576 - val_loss: 12.3190 - val_acc: 0.2424\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 0s 187us/step - loss: 11.2979 - acc: 0.2576 - val_loss: 12.3186 - val_acc: 0.2424\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 0s 157us/step - loss: 11.2952 - acc: 0.2576 - val_loss: 12.3177 - val_acc: 0.2424\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 0s 147us/step - loss: 11.2892 - acc: 0.2576 - val_loss: 12.3174 - val_acc: 0.2424\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 0s 212us/step - loss: 11.2871 - acc: 0.2576 - val_loss: 12.3170 - val_acc: 0.2424\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 0s 160us/step - loss: 11.2832 - acc: 0.2576 - val_loss: 12.3166 - val_acc: 0.2424\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 0s 178us/step - loss: 11.2803 - acc: 0.2576 - val_loss: 12.3161 - val_acc: 0.2424\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 0s 139us/step - loss: 11.2766 - acc: 0.2576 - val_loss: 12.3152 - val_acc: 0.2424\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 0s 168us/step - loss: 11.2688 - acc: 0.2576 - val_loss: 12.3143 - val_acc: 0.2424\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 0s 253us/step - loss: 11.2629 - acc: 0.2576 - val_loss: 12.3141 - val_acc: 0.2424\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 0s 223us/step - loss: 11.2613 - acc: 0.2576 - val_loss: 12.3138 - val_acc: 0.2424\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 0s 249us/step - loss: 11.2594 - acc: 0.2576 - val_loss: 12.3135 - val_acc: 0.2424\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 0s 196us/step - loss: 11.2560 - acc: 0.2576 - val_loss: 12.3131 - val_acc: 0.2424\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 0s 285us/step - loss: 11.2535 - acc: 0.2576 - val_loss: 12.3124 - val_acc: 0.2424\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 0s 179us/step - loss: 11.2477 - acc: 0.2576 - val_loss: 12.3121 - val_acc: 0.2424\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 0s 133us/step - loss: 11.2450 - acc: 0.2576 - val_loss: 12.3116 - val_acc: 0.2424\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 0s 224us/step - loss: 11.2423 - acc: 0.2576 - val_loss: 12.3112 - val_acc: 0.2424\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 0s 91us/step - loss: 11.2389 - acc: 0.2576 - val_loss: 12.3107 - val_acc: 0.2424\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 0s 163us/step - loss: 11.2345 - acc: 0.2576 - val_loss: 12.3102 - val_acc: 0.2424\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 0s 135us/step - loss: 11.2304 - acc: 0.2576 - val_loss: 12.3096 - val_acc: 0.2424\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 0s 102us/step - loss: 11.2243 - acc: 0.2576 - val_loss: 12.3090 - val_acc: 0.2424\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 0s 108us/step - loss: 11.2198 - acc: 0.2576 - val_loss: 12.3082 - val_acc: 0.2424\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 0s 154us/step - loss: 11.2111 - acc: 0.2576 - val_loss: 12.3077 - val_acc: 0.2424\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 0s 144us/step - loss: 11.2076 - acc: 0.2576 - val_loss: 12.3072 - val_acc: 0.2424\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 0s 139us/step - loss: 11.2030 - acc: 0.2576 - val_loss: 12.3063 - val_acc: 0.2424\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 0s 207us/step - loss: 11.1889 - acc: 0.2576 - val_loss: 12.3056 - val_acc: 0.2424\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 0s 210us/step - loss: 11.1826 - acc: 0.2576 - val_loss: 12.3054 - val_acc: 0.2424\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 0s 135us/step - loss: 11.1803 - acc: 0.2576 - val_loss: 12.3050 - val_acc: 0.2424\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 0s 175us/step - loss: 11.1778 - acc: 0.2576 - val_loss: 12.3047 - val_acc: 0.2424\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 0s 153us/step - loss: 11.1745 - acc: 0.2576 - val_loss: 12.3039 - val_acc: 0.2424\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 0s 171us/step - loss: 11.1677 - acc: 0.2576 - val_loss: 12.3036 - val_acc: 0.2424\n",
      "Epoch 55/100\n",
      "66/66 [==============================] - 0s 109us/step - loss: 11.1647 - acc: 0.2576 - val_loss: 12.3033 - val_acc: 0.2424\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 0s 99us/step - loss: 11.1615 - acc: 0.2576 - val_loss: 12.3029 - val_acc: 0.2424\n",
      "Epoch 57/100\n",
      "66/66 [==============================] - 0s 240us/step - loss: 11.1576 - acc: 0.2576 - val_loss: 12.3025 - val_acc: 0.2424\n",
      "Epoch 58/100\n",
      "66/66 [==============================] - 0s 103us/step - loss: 11.1505 - acc: 0.2576 - val_loss: 12.3020 - val_acc: 0.2424\n",
      "Epoch 59/100\n",
      "66/66 [==============================] - 0s 99us/step - loss: 11.1457 - acc: 0.2576 - val_loss: 12.3015 - val_acc: 0.2424\n",
      "Epoch 60/100\n",
      "66/66 [==============================] - 0s 92us/step - loss: 11.1395 - acc: 0.2576 - val_loss: 12.3007 - val_acc: 0.2424\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 169us/step - loss: 11.1295 - acc: 0.2576 - val_loss: 12.3003 - val_acc: 0.2424\n",
      "Epoch 62/100\n",
      "66/66 [==============================] - 0s 147us/step - loss: 11.1264 - acc: 0.2576 - val_loss: 12.2999 - val_acc: 0.2424\n",
      "Epoch 63/100\n",
      "66/66 [==============================] - 0s 115us/step - loss: 11.1196 - acc: 0.2576 - val_loss: 12.2995 - val_acc: 0.2424\n",
      "Epoch 64/100\n",
      "66/66 [==============================] - 0s 137us/step - loss: 11.1155 - acc: 0.2576 - val_loss: 12.2991 - val_acc: 0.2424\n",
      "Epoch 65/100\n",
      "66/66 [==============================] - 0s 127us/step - loss: 11.1108 - acc: 0.2576 - val_loss: 12.2986 - val_acc: 0.2424\n",
      "Epoch 66/100\n",
      "66/66 [==============================] - 0s 95us/step - loss: 11.1060 - acc: 0.2576 - val_loss: 12.2982 - val_acc: 0.2424\n",
      "Epoch 67/100\n",
      "66/66 [==============================] - 0s 125us/step - loss: 11.0958 - acc: 0.2576 - val_loss: 12.2977 - val_acc: 0.2424\n",
      "Epoch 68/100\n",
      "66/66 [==============================] - 0s 121us/step - loss: 11.0882 - acc: 0.2576 - val_loss: 12.2969 - val_acc: 0.2424\n",
      "Epoch 69/100\n",
      "66/66 [==============================] - 0s 103us/step - loss: 11.0789 - acc: 0.2576 - val_loss: 12.2965 - val_acc: 0.2424\n",
      "Epoch 70/100\n",
      "66/66 [==============================] - 0s 170us/step - loss: 11.0737 - acc: 0.2576 - val_loss: 12.2961 - val_acc: 0.2424\n",
      "Epoch 71/100\n",
      "66/66 [==============================] - 0s 123us/step - loss: 11.0690 - acc: 0.2576 - val_loss: 12.2956 - val_acc: 0.2424\n",
      "Epoch 72/100\n",
      "66/66 [==============================] - 0s 127us/step - loss: 11.0640 - acc: 0.2576 - val_loss: 12.2952 - val_acc: 0.2424\n",
      "Epoch 73/100\n",
      "66/66 [==============================] - 0s 116us/step - loss: 11.0575 - acc: 0.2576 - val_loss: 12.2947 - val_acc: 0.2424\n",
      "Epoch 74/100\n",
      "66/66 [==============================] - 0s 136us/step - loss: 11.0468 - acc: 0.2576 - val_loss: 12.2942 - val_acc: 0.2424\n",
      "Epoch 75/100\n",
      "66/66 [==============================] - 0s 115us/step - loss: 11.0407 - acc: 0.2576 - val_loss: 12.2938 - val_acc: 0.2424\n",
      "Epoch 76/100\n",
      "66/66 [==============================] - 0s 105us/step - loss: 11.0347 - acc: 0.2576 - val_loss: 12.2933 - val_acc: 0.2424\n",
      "Epoch 77/100\n",
      "66/66 [==============================] - 0s 109us/step - loss: 11.0268 - acc: 0.2576 - val_loss: 12.2924 - val_acc: 0.2424\n",
      "Epoch 78/100\n",
      "66/66 [==============================] - 0s 95us/step - loss: 11.0122 - acc: 0.2576 - val_loss: 12.2922 - val_acc: 0.2424\n",
      "Epoch 79/100\n",
      "66/66 [==============================] - 0s 143us/step - loss: 11.0088 - acc: 0.2576 - val_loss: 12.2919 - val_acc: 0.2424\n",
      "Epoch 80/100\n",
      "66/66 [==============================] - 0s 114us/step - loss: 10.9996 - acc: 0.2576 - val_loss: 12.2915 - val_acc: 0.2424\n",
      "Epoch 81/100\n",
      "66/66 [==============================] - 0s 102us/step - loss: 10.9949 - acc: 0.2576 - val_loss: 12.2912 - val_acc: 0.2424\n",
      "Epoch 82/100\n",
      "66/66 [==============================] - 0s 88us/step - loss: 10.9883 - acc: 0.2576 - val_loss: 12.2908 - val_acc: 0.2424\n",
      "Epoch 83/100\n",
      "66/66 [==============================] - 0s 94us/step - loss: 10.9827 - acc: 0.2576 - val_loss: 12.2901 - val_acc: 0.2424\n",
      "Epoch 84/100\n",
      "66/66 [==============================] - 0s 84us/step - loss: 10.9705 - acc: 0.2576 - val_loss: 12.2898 - val_acc: 0.2424\n",
      "Epoch 85/100\n",
      "66/66 [==============================] - 0s 152us/step - loss: 10.9661 - acc: 0.2576 - val_loss: 12.2895 - val_acc: 0.2424\n",
      "Epoch 86/100\n",
      "66/66 [==============================] - 0s 124us/step - loss: 10.9611 - acc: 0.2576 - val_loss: 12.2891 - val_acc: 0.2424\n",
      "Epoch 87/100\n",
      "66/66 [==============================] - 0s 129us/step - loss: 10.9519 - acc: 0.2576 - val_loss: 12.2885 - val_acc: 0.2424\n",
      "Epoch 88/100\n",
      "66/66 [==============================] - 0s 124us/step - loss: 10.9405 - acc: 0.2576 - val_loss: 12.2882 - val_acc: 0.2424\n",
      "Epoch 89/100\n",
      "66/66 [==============================] - 0s 103us/step - loss: 10.9359 - acc: 0.2576 - val_loss: 12.2879 - val_acc: 0.2424\n",
      "Epoch 90/100\n",
      "66/66 [==============================] - 0s 101us/step - loss: 10.9308 - acc: 0.2576 - val_loss: 12.2875 - val_acc: 0.2424\n",
      "Epoch 91/100\n",
      "66/66 [==============================] - 0s 118us/step - loss: 10.9251 - acc: 0.2576 - val_loss: 12.2871 - val_acc: 0.2424\n",
      "Epoch 92/100\n",
      "66/66 [==============================] - 0s 164us/step - loss: 10.9163 - acc: 0.2576 - val_loss: 12.2868 - val_acc: 0.2424\n",
      "Epoch 93/100\n",
      "66/66 [==============================] - 0s 115us/step - loss: 10.9106 - acc: 0.2576 - val_loss: 12.2864 - val_acc: 0.2424\n",
      "Epoch 94/100\n",
      "66/66 [==============================] - 0s 142us/step - loss: 10.9038 - acc: 0.2576 - val_loss: 12.2860 - val_acc: 0.2424\n",
      "Epoch 95/100\n",
      "66/66 [==============================] - 0s 116us/step - loss: 10.8973 - acc: 0.2576 - val_loss: 12.2856 - val_acc: 0.2424\n",
      "Epoch 96/100\n",
      "66/66 [==============================] - 0s 143us/step - loss: 10.8883 - acc: 0.2576 - val_loss: 12.2838 - val_acc: 0.2424\n",
      "Epoch 97/100\n",
      "66/66 [==============================] - 0s 108us/step - loss: 10.8740 - acc: 0.2576 - val_loss: 12.2812 - val_acc: 0.2424\n",
      "Epoch 98/100\n",
      "66/66 [==============================] - 0s 92us/step - loss: 10.8635 - acc: 0.2576 - val_loss: 12.2800 - val_acc: 0.2424\n",
      "Epoch 99/100\n",
      "66/66 [==============================] - 0s 126us/step - loss: 10.8574 - acc: 0.2576 - val_loss: 12.2788 - val_acc: 0.2424\n",
      "Epoch 100/100\n",
      "66/66 [==============================] - 0s 112us/step - loss: 10.8526 - acc: 0.2576 - val_loss: 12.2774 - val_acc: 0.2424\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2,  # output dim is 2, one score per each class\n",
    "                activation='softmax',\n",
    "                kernel_regularizer=L1L2(l1=0.0, l2=0.1),\n",
    "                input_dim=1))  # input dimension = number of features your data has\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))\n",
    "\n",
    "def Prediction(n):\n",
    "    p = model.predict(x=[[n]])\n",
    "    return [0 if i < 0.5 else 1 for i in p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number 1 is [0, 1] prime\n",
      "The number 2 is [0, 1] prime\n",
      "The number 3 is [0, 1] prime\n",
      "The number 4 is [0, 1] prime\n",
      "The number 5 is [0, 1] prime\n",
      "The number 6 is [0, 1] prime\n",
      "The number 7 is [0, 1] prime\n",
      "The number 8 is [0, 1] prime\n",
      "The number 9 is [0, 1] prime\n",
      "The number 10 is [0, 1] prime\n",
      "The number 11 is [0, 1] prime\n",
      "The number 12 is [0, 1] prime\n",
      "The number 13 is [0, 1] prime\n",
      "The number 14 is [0, 1] prime\n",
      "The number 15 is [0, 1] prime\n",
      "The number 16 is [0, 1] prime\n",
      "The number 17 is [0, 1] prime\n",
      "The number 18 is [0, 1] prime\n",
      "The number 19 is [0, 1] prime\n",
      "The number 20 is [0, 1] prime\n",
      "The number 21 is [0, 1] prime\n",
      "The number 22 is [0, 1] prime\n",
      "The number 23 is [0, 1] prime\n",
      "The number 24 is [0, 1] prime\n",
      "The number 25 is [0, 1] prime\n",
      "The number 26 is [0, 1] prime\n",
      "The number 27 is [0, 1] prime\n",
      "The number 28 is [0, 1] prime\n",
      "The number 29 is [0, 1] prime\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 30):\n",
    "    print(\"The number {0} is {1} prime\".format(n, Prediction(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(1, 100)]\n",
    "Y = [isPrime(i) for i in Xt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {'oculta': tf.Variable(tf.random_normal([1, 3]), name='w_oculta'),\n",
    "     'saida': tf.Variable(tf.random_normal([3, 1]), name='w_saida')}\n",
    "b = {'oculta': tf.Variable(tf.random_normal([3]), name='b_oculta'),\n",
    "     'saida': tf.Variable(tf.random_normal([1]), name='b_saida')}\n",
    "\n",
    "xph = tf.placeholder(tf.float32, [99, 1], name='xph')\n",
    "yph = tf.placeholder(tf.float32, [99,], name='yph')\n",
    "\n",
    "oculta = tf.add(tf.matmul(xph, W['oculta']), b['oculta'])\n",
    "ativacao_oculta = tf.sigmoid(oculta)\n",
    "saida = tf.add(tf.matmul(ativacao_oculta, W['saida']), b['saida'])\n",
    "ativacao_saida = tf.sigmoid(saida)\n",
    "\n",
    "err = tf.losses.mean_squared_error(yph, ativacao_saida)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(err)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (99,) for Tensor 'xph_9:0', which has shape '(99, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ac4297d8d0a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepocas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmean_erro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcusto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepocas\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcusto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/machine-learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/machine-learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (99,) for Tensor 'xph_9:0', which has shape '(99, 1)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    for epocas in range(100):\n",
    "        mean_erro = 0\n",
    "        _, custo = s.run([opt, err], feed_dict={xph: X, yph: Y})\n",
    "        if epocas % 20 == 0:\n",
    "            print(custo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
